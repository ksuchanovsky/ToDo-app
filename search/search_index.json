{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Introduction: Rehost - jAWS Spring Boot with PostgreSQL","text":"<p>All the resources will be created in Frankfurt (eu-central-1) region.</p>"},{"location":"budget/","title":"Introduction: Setup budget cost in AWS","text":"<p>AWS BUDGET - set custom budgets to track your costs and usage, and respond quickly to alerts received from email or SNS notifications if you exceed your threshold.</p> <ul> <li>AWS - BUDGET LINK </li> </ul> <p>By default, available only for root user, can be enabled for IAM in root account section</p> <p>AWS predefined templates: </p> <ul> <li>Zero spend ( spending &gt; 0.01$)</li> <li>Monthly costs</li> <li>Daily Savings Plans coverage (fall below defined target)</li> <li>Daily reservation utilization (alerts when utilization of instance is below target)</li> </ul> <p>Scope All AWS services are in scope in  budget.</p>"},{"location":"budget/budget/","title":"Set up Budget","text":"<p>Budgets</p> <ul> <li>Find BUDGET service</li> <li> <p>Click on \"Create budget\"</p> </li> <li> <p></p> </li> <li> <p>Two options - use template or custom , we will create custom budget setup, click on Customize</p> </li> <li> <p>Choose \"Cost budget - Recommended\" - we will monitor our costs in monthly or daily period</p> </li> <li> <p></p> </li> <li> <p>Fill budget name, choose period (time range of monitoring) - monthly , choose start date, </p> </li> <li>Enter your budgeted amount - 5$. We have budget 5$ for month.</li> <li>Choose scope options if you would like to filter specific AWS cost</li> <li>Click on next</li> <li> <p>Add alert threshold, when alert should be triggered. E.g-80% in case of 5$ = when costs &gt; 4$ then email will be sent</p> </li> <li> <p></p> </li> <li> <p>Review budget alert and create it</p> </li> </ul>"},{"location":"budget/budget/#email-notification","title":"Email notification","text":"<p>When threshold is reached then you will get email from AWS </p> <p></p>"},{"location":"budget/prerequisites/","title":"Prerequisites","text":"<p>Optional - Enable aws budget for IAM users  - log in as root - Click on Account</p> <ul> <li> <p></p> </li> <li> <p>Find section - \"IAM User and Role Access to Billing Information\"</p> </li> <li>Clink on edit</li> <li> <p>Enable - \"Activate IAM Access\"</p> </li> <li> <p></p> </li> <li>In addition to activating IAM access, you must also attach the required IAM policies to those users or roles</li> <li>Attach policy : AWSBudgetsActionsWithAWSResourceControlAccess  to user/groups.</li> <li>Log out from root account</li> <li>Sign in as IAM user</li> </ul>"},{"location":"budget/recap/","title":"Recap","text":""},{"location":"budget/recap/#what-has-been-done-and-what-has-we-achieved","title":"What has been done and what has we achieved?","text":"<ol> <li>Usage of AWS BUDGET service</li> <li>Enable AWS  budget for IAM users </li> <li>Create IAM policies that grant permissions to billing data</li> <li>Create budget manually/template</li> <li>Alert threshold (email)</li> </ol>"},{"location":"codePipeline/","title":"Introduction: Build pipeline using Code Pipeline","text":"<p>All the resources will be created in Frankfurt (eu-central-1) region.</p> <p></p>"},{"location":"codePipeline/cleanup/","title":"Cleanup","text":"<ol> <li>Delete Build projects in CodeBuild</li> <li>Delete Pipelines in CodePipeline</li> <li>Delete application in Beanstalk</li> <li>Terminate environments in Beanstalk</li> <li>Remove all deployed objects in S3 and delete bucket</li> </ol>"},{"location":"codePipeline/codebuild/codeBuild/","title":"Code Build","text":"<p>We will be creating two build projects (server/client) and also two pipelines (client/server)</p>"},{"location":"codePipeline/codebuild/codeBuild/#build-project-for-server-part","title":"Build project for server part","text":"<ol> <li> <p>Open the AWS CodeBuild console at https://console.aws.amazon.com/codesuite/codebuild/home.</p> </li> <li> <p>If a CodeBuild information page is displayed, choose Create build project.  Otherwise, on the navigation pane, expand Build, choose Build projects, and then choose Create build project.</p> </li> <li> <p>Choose Create build project.</p> </li> <li> <p>In Project configuration:</p> <p>Project name</p> <p>In Project name, enter a name for this build project. Build project names must be unique across each AWS account.</p> </li> <li> <p>In Source:</p> <p>Source provider   Choose GitHub. In Repository choose Connect with a GitHub personal access token.</p> <p></p> <p>For Personal Access Token, paste the token you copied to your clipboard and choose Save Token.</p> <p>Repository URL   In Repository URL, enter the URL for your GitHub repository.</p> <p></p> <p>In Source version set branch for refactor_dynamodb.</p> </li> <li> <p>In Primary source webhook events, select the following.</p> <p>Rebuild every time a code change is pushed to this repository.</p> <p>From Event type choose PUSH.</p> <p></p> </li> <li> <p>In Environment, select the following.</p> <ul> <li>Environment image - Managed image</li> <li>Operating system - Amazon Linux 2</li> <li>Runtime - Standard</li> <li>Image - aws/codebuild/amazonlinux2-x86_64-standard:4.0</li> <li>Image version - Always use the latest image for this runtime version</li> <li>Environment type - Linux</li> <li>Service role - New service role (role name is generated automatically)</li> </ul> <p></p> </li> <li> <p>In Buildspec, select the following.</p> <ul> <li>Choose Use a buildspec file to use the buildspec.yml file for server in the source code root directory.</li> <li>Define name for buidspec file buildspec_spring-boot-server.yml.</li> </ul> <p></p> </li> <li> <p>Choose Create build project.</p> </li> <li> <p>In Github settings in part Webhooks you should see new webhook.</p> <p></p> </li> <li> <p>Return to your CodeBuild project and start build.</p> </li> <li> <p>In tab Phase details and Build logs you can see details for running build. </p> <p></p> </li> </ol>"},{"location":"codePipeline/codebuild/codeBuild/#build-project-for-client-part","title":"Build project for client part","text":"<ol> <li>In Project name, enter a name for this build project. Build project names must be unique</li> <li>Buildspec section<ul> <li>Choose Use a buildspec file to use the buildspec.yml file for client in the source code root directory.</li> <li>Define name for buidspec file buildspec_angular-11-client.yml.</li> </ul> </li> <li>All other options are same as in build project for server part.</li> </ol>"},{"location":"codePipeline/codepipeline/codePipeline/","title":"CodePipeline","text":""},{"location":"codePipeline/codepipeline/codePipeline/#code-pipeline-for-client-part","title":"Code pipeline for client part","text":"<p>For deployment of client part we need S3 bucket. Go to section  S3</p> <ol> <li> <p>Open the CodePipeline console at http://console.aws.amazon.com/codesuite/codepipeline/home.</p> </li> <li> <p>On the Welcome page, Getting started page, or the Pipelines page, choose Create pipeline.</p> </li> <li> <p>In Step 1: Choose pipeline settings, in Pipeline name, enter pipeline name.</p> </li> <li> <p>In Service role, choose New service role to allow CodePipeline to create a new service role in IAM.</p> </li> <li> <p>In Step 2: Add source stage, in Source provider, choose GitHub (Version 2).</p> </li> <li> <p>Under Connection, if you have not already created a connection to your provider,      choose Connect to GitHub. </p> </li> <li> <p>Create a connection.</p> <ul> <li>In Connection name, enter the name for your connection.</li> </ul> <p></p> <ul> <li>Choose Connect to GitHub. </li> <li>Install a new app.</li> </ul> <p></p> <ul> <li> <p>Choose your GitHub account</p> </li> <li> <p>If prompted, on the GitHub login page, sign in with your GitHub credentials.</p> </li> <li>If you have multiple organizations, you might be prompted to choose the organization where you want to install the app.</li> <li>Choose the repository settings where you want to install the app. Choose Install. </li> </ul> <p></p> <ul> <li>After connection is succesfull you should see that GitHub is ready for use.</li> </ul> <p></p> </li> <li> <p>In Repository name, choose the name of your third-party repository. </p> </li> <li> <p>In Branch name, choose the branch where you want your pipeline to detect source changes.</p> </li> <li> <p>Select Start the pipeline on source code change.</p> <p>Note</p> <p>The action accesses the files from the GitHub repository  and stores the artifacts in a ZIP file in the pipeline artifact store.</p> </li> <li> <p>In Step 3: Add Build provider, choose AWS CodeBuild</p> </li> <li> <p>In Project name, enter the name of the project for client you provided in build projects.</p> </li> <li> <p>In Environment variables introduce new environment variable: SPRING_PROFILES_ACTIVE with value: prod</p> </li> <li> <p>In Step 4: In Add deploy stage, for Deploy provider, choose Amazon S3.</p> </li> <li> <p>For Bucket, choose the Bucket you created.</p> </li> <li> <p>Select Extract file before deploy. </p> </li> <li> <p>After the pipeline has run successfully, you can see the result in a web browser. </p> </li> </ol> <p></p> <p>Note</p> <p>The deployment fails if you do not select Extract file before deploy.  This is because the AWS CodeCommit action in your pipeline zips source artifacts and your file is a ZIP file.</p>"},{"location":"codePipeline/codepipeline/codePipeline/#code-pipeline-for-server-part","title":"Code pipeline for server part","text":"<ol> <li> <p>Procedure is the same as for client part, only with small changes in options for build and deploy stage.</p> </li> <li> <p>In Add Build provider, choose AWS CodeBuild</p> </li> <li> <p>In Project name, enter the name of the project for server you provided in build projects.</p> </li> <li> <p>On Add deploy stage, for Deploy provider, choose AWS Elastic Beanstalk.</p> </li> <li> <p>For Application name, choose the Elastic Beanstalk application you just created.</p> </li> <li> <p>For Environment name, choose the environment you just created.</p> </li> </ol>"},{"location":"codePipeline/prerequisites/prerequisites/","title":"Prerequisites","text":"<p>Generate a personal access token for your CodeBuild project.  We recommend that you create a GitHub Enterprise user and generate a personal access token for this user.  Copy it to your clipboard so that it can be used when you create your CodeBuild project. </p> <ul> <li>In GitHub open settings -&gt; Developer settings -&gt; Personal Access tokens</li> <li>Click on Generate new token </li> </ul> <p></p> <ul> <li>When you create the personal access token, include the repo scope in the definition. </li> </ul> <p></p> <ul> <li>Save generated personal access token </li> </ul>"},{"location":"codePipeline/recap/","title":"Recap","text":""},{"location":"codePipeline/recap/#what-has-been-done-and-what-has-we-achieved","title":"What has been done and what has we achieved?","text":"<ol> <li>Used AWS CodeBuild to compile application from repository</li> <li>Used AWS CodePipeline to automate building and deploying ToDo application</li> </ol>"},{"location":"dynamoDB/","title":"Introduction: Refactor - jAWS Spring Boot with DynamoDB","text":"<p>All the resources will be created in Frankfurt (eu-central-1) region.</p> <p></p>"},{"location":"dynamoDB/asg/","title":"Scheduled Scaling","text":"<p>One of the major advantages of the public cloud usage is the option to scale the infrastructure, based on the current demand. To save the costs, we'll have a look at the horizontal scaling scenario, the scale-down option.</p>"},{"location":"dynamoDB/asg/#scheduled-scaling-of-the-ec2-instances","title":"Scheduled Scaling of the EC2 instances","text":"<ol> <li>Navigate to: EC2 -&gt; Auto Scaling groups (ASGs)</li> <li>Click the name of specific ASG</li> <li>Pick Automatic Scaling tab</li> <li>Scroll down and in the Scheduled Actions section click Button: Create scheduled action</li> <li>Name the rule and pick the timeframe close to current one (e.g. in 2 min), to see scaling happen. Setting the Capacity: Desired = Min = Max = 0. </li> <li>Wait for the rule to be applied (check the Activity -&gt; Activity History): </li> <li>Create another rule, mame the rule and pick the timeframe close to current one (e.g. in 2 min). Setting the Capacity back to: Desired = Min = Max = 1 </li> <li>Wait for the rule to be applied (check the Activity -&gt; Activity History): </li> </ol>"},{"location":"dynamoDB/beanstalk/","title":"Elastic Beanstalk","text":""},{"location":"dynamoDB/beanstalk/#high-level-view","title":"High-level view","text":""},{"location":"dynamoDB/beanstalk/#create-environment","title":"Create Environment","text":"<ol> <li>Open Elastic Beanstalk console at https://console.aws.amazon.com/elasticbeanstalk/.</li> <li>On the application overview page, choose Create a new environment.</li> <li>For environment tier, choose the Web server environment.</li> <li>Enter an application name.</li> <li>Choose platform Java. </li> <li>Application code \u2013 choose \u201cUpload your code\u201d.</li> <li>Upload builded application (jar file).  </li> <li>In section Presets choose High availability and choose Next.</li> <li>The Configure service access page displays.</li> <li>Choose Create and use new service role for Service Role.</li> <li>Enter Service role name.</li> <li>If the EC2 instance profile dropdown list doesn't list any values to choose from,      Choose View permission details. This displays under the EC2 instance profile dropdown list</li> <li>A modal window titled View instance profile permissions displays. This window lists the managed profiles that you'll need to      attach to the new EC2 instance profile that you create. It also provides a link to launch the IAM console.</li> <li>Choose the IAM console link displayed at the top of the window.</li> <li>In the IAM console navigation pane, choose Roles.</li> <li>Choose Create role.</li> <li>Under Trusted entity type, choose AWS service.</li> <li>Under Use case, choose EC2.</li> <li>Choose Next.</li> <li> <p>Attach the appropriate managed policies. Scroll in the View instance profile permissions modal window to see the managed policies.      The policies are also listed here:</p> <pre><code>AWSElasticBeanstalkWebTier\n\nAmazonDynamoDBFullAccess\n\nAmazonRDSDataFullAccess\n\nSecretsManagerReadWrite\n\nCloudWatchFullAccess\n</code></pre> </li> <li> <p>Choose Next and Enter a name for the role.</p> </li> <li>Choose Create role.</li> <li>Return to the Elastic Beanstalk console window that is open.</li> <li>Close the modal window View instance profile permissions.</li> <li> <p>Choose refresh icon, next to the EC2 instance profile dropdown list.     This refreshes the dropdown list, so that the Role you just created will display in the dropdown list.</p> <p></p> </li> <li> <p>The default settings remain in  Set up networking, database, and tags step.</p> </li> <li>Step Configure instance traffic and scaling.</li> <li>In Section Capacity set Environment type to Load balanced.</li> <li> <p>Set Minimum number of instances to 2 and maximum for instances to 3.</p> <p></p> </li> <li> <p>Set Instance types: t2.micro.</p> </li> <li>Set Availability Zones: Any 1.</li> <li> <p>Set Placement: eu-central-1.</p> <p></p> </li> <li> <p>In section Load Balancer Type choose load balancer type Application Load Balancer.</p> </li> <li> <p>In part Processes change this values:</p> <ul> <li>Port      : 8080</li> <li>HTTP code : 200-299</li> <li>Health check path : /api/tutorials </li> </ul> </li> <li> <p>Choose Next.</p> </li> <li>Step Configure updates, monitoring, and logging.</li> <li> <p>In Section Health reporting set System to Basic.</p> <p> </p> </li> <li> <p>In Section Rolling updates and deployments set Deployment policy to All at once.</p> </li> <li>Set Ignore health check to True</li> <li> <p>Enable streaming of the logs to Cloudwatch, by setting the checkbox      (We will need this for later analysis in in part Logs analysis using CloudWatch):</p> <p></p> </li> <li> <p>In section Environment properties introduce new environment variable: SPRING_PROFILES_ACTIVE with value: prod.          </p> </li> <li> <p>Check Review page and submit.</p> </li> <li>If you're using AWS Academy account</li> </ol> <p>Note</p> <p>This will take a few minutes. Beanstalk will create EC2 instance, Elastic load balancer,  Auto Scaling Group, Target group, Security Group, CloudWatch Alarm. </p>"},{"location":"dynamoDB/cleanup/","title":"Cleanup","text":"<ol> <li>Remove DynamoDB table</li> <li>Remove all deployed objects in S3 and delete bucket</li> <li>Remove Beanstalk Environment</li> <li>Disable and delete CloudFront distribution</li> </ol>"},{"location":"dynamoDB/cloud_watch/","title":"Logs analysis using CloudWatch","text":""},{"location":"dynamoDB/cloud_watch/#high-level-view","title":"High-level view","text":""},{"location":"dynamoDB/cloud_watch/#display-the-application-logfiles-using-cloudwatch","title":"Display the application logfiles using CloudWatch","text":"<ul> <li>Go to Amazon CloudWatch service</li> <li>Open Logs insight and for the log groups pick: /aws/beanstalk/.../var/log/web.stdout.log and in the query change limit to 2000.</li> <li>Hit Run Query button afterwards (please note there is an option in the upper right corner to change the time range of the displayed logs) </li> </ul>"},{"location":"dynamoDB/cloudfront/","title":"CloudFront","text":""},{"location":"dynamoDB/cloudfront/#high-level-view","title":"High-level view","text":""},{"location":"dynamoDB/cloudfront/#create-cloudfront","title":"Create CloudFront","text":"<ol> <li>Open the Amazon CloudFront console at https://console.aws.amazon.com/cloudfront/.</li> <li>Create distribution.</li> <li>Set origin domain for client - find S3 Bucket.</li> <li>Origin access set to Origin access control settings.</li> <li>For Origin access control Create new control setting.    CloudFront will provide us policy statement for bucket policy after creating the distribution.</li> <li>For Allowed HTTP methods choose GET, HEAD, OPTIONS, PUT, POST, PATCH, DELETE.    </li> <li>In section Cache key and origin requests use recommended cache policy and disable caching and set     Origin request policy to CORS-S3Origin.    </li> <li>Default root object set to index.html.</li> <li>Create distribution.</li> <li>After distribution is created pop up window offer us new bucket policy. Copy it and replace existing policy in our S3 bucket.  </li> <li>In Origins section create origin.</li> <li>Set origin domain for application load balancer - find ELB.</li> <li>Create origin.</li> <li>In Behaviors section create behaviour.</li> <li>Set Path pattern to /api/*</li> <li>Set Origin and origin groups to ELB.</li> <li>For Allowed HTTP methods choose GET, HEAD, OPTIONS, PUT, POST, PATCH, DELETE.</li> <li>In section Cache key and origin requests use recommended cache policy and disable caching and set      Origin request policy to AllViewer. </li> <li>After behaviour is created distribution is redeployed. After succesfull deployment open website with distribution domain name.  (General section)</li> </ol>"},{"location":"dynamoDB/dynamodb/","title":"DynamoDB","text":""},{"location":"dynamoDB/dynamodb/#high-level-view","title":"High-level view","text":""},{"location":"dynamoDB/dynamodb/#create-dynamodb-table","title":"Create DynamoDB table","text":"<ol> <li> <p>Open the Amazon DynamoDB console at https://console.aws.amazon.com/dynamodb.</p> </li> <li> <p>Create table.</p> </li> <li> <p>Enter table name: tutorials.</p> </li> <li> <p>Enter partition key: id.</p> </li> </ol> <p></p>"},{"location":"dynamoDB/localstack/","title":"Building the projects","text":""},{"location":"dynamoDB/localstack/#retrieve-project-sources","title":"Retrieve project sources","text":"<p>Clone project from github.ibm.com and switch to lift_and_switch branch</p> <pre><code>git clone git@github.ibm.com:jAWS/ToDo-app.git\ncd ToDo-app\ngit switch lift_and_switch\ngit status\n</code></pre> <p>Result:</p> <pre><code>  On branch lift_and_shift\n  Your branch is up to date with 'origin/lift_and_shift'.\n</code></pre>"},{"location":"dynamoDB/localstack/#building-the-backend-app","title":"Building the backend app","text":"<ol> <li> <p>To build run: </p> <pre><code>cd spring-boot-server\nmvn package\n</code></pre> </li> <li> <p>Built artifact is present in: <code>target/*.jar</code> file</p> </li> </ol>"},{"location":"dynamoDB/localstack/#building-the-frontend-app","title":"Building the frontend app","text":"<ol> <li> <p>Have NodeJS installed (LTS version is preffered, 18.12.1 is latest LTS). I have 16.15.0, this is version about 6 months old, should work just fine.</p> </li> <li> <p>Install angular CLI globaly with npm (node package manager) -&gt; if you have yarn, you can use that.</p> <pre><code>npm install -g @angular/cli\n</code></pre> </li> <li> <p>Clone GitHub repository and navigate to angular app application is located in angular-11-client/ folder.</p> </li> <li> <p>Install application\u2019s dependencies</p> <pre><code>npm install\n</code></pre> </li> <li> <p>To build application, simply run:</p> <pre><code>ng build\n</code></pre> <p>You can also run ng build --prod, which will produce optimized production bundle.</p> <p>Difference between them is first one also has map files to allow tracing errors and issues to specific code.</p> <p>Production bundle will just produce mostly useless stacktrace, as compiled JS code is optimized for performance.</p> </li> </ol> <p>Produced bundle can be found in dist/Angular11Crud/ folder, you can just copy-paste this bundle into AWS S3 bucket more info can be found here https://levelup.gitconnected.com/learn-how-to-create-and-deploy-the-angular-application-to-aws-serverless-s3-81f8a838b563</p>"},{"location":"dynamoDB/localstack/#running-localy-optional-for-next-steps","title":"Running localy (optional for next steps)","text":""},{"location":"dynamoDB/localstack/#starting-localstack","title":"Starting Localstack","text":"<p>To be able to run the app locally, run Localstack using docker-compose (alternatively run podman-compose cmd instead of docker-compoose) first:</p> <pre><code>cd docker-compose\ndocker-compose up\n</code></pre>"},{"location":"dynamoDB/localstack/#running-the-backend-app-locally","title":"Running the backend app locally","text":"<ol> <li> <p>Run (in project root folder): </p> <pre><code>cd spring-boot-server\nmvn spring-boot:run\n</code></pre> </li> <li> <p>To prove deployment suceeded, run:</p> <pre><code>curl http://localhost:8080/api/tutorials\n</code></pre> </li> </ol>"},{"location":"dynamoDB/localstack/#running-the-frontend-app","title":"Running the frontend app","text":"<ol> <li> <p>Run:</p> <pre><code>npm run start\n</code></pre> <p>(which runs ng serve internally)</p> <p>OR </p> <pre><code>ng serve\n</code></pre> </li> </ol> <p>Application is then started on port defined in angular.json (by default 4200).</p>"},{"location":"dynamoDB/recap/","title":"Recap","text":""},{"location":"dynamoDB/recap/#what-has-been-done-and-what-has-we-achieved","title":"What has been done and what has we achieved?","text":"<ol> <li>Cloned the project from repo from refactor_dynamodb branch</li> <li>Created DynamoDB database to store the application data   </li> <li>Created S3 bucket to host frontend</li> <li>Used Beanstalk to for easy application deployment</li> <li>Used CloudFront for distribution static and dynamic web content.</li> <li>Used CloudWatch to analyze logs</li> </ol>"},{"location":"dynamoDB/s3/","title":"S3","text":""},{"location":"dynamoDB/s3/#high-level-view","title":"High-level view","text":""},{"location":"dynamoDB/s3/#create-bucket","title":"Create bucket","text":"<ol> <li>Open the Amazon S3 console at https://console.aws.amazon.com/s3/.</li> <li>Create bucket.</li> <li>Choose unique bucket name.</li> <li>AWS Region = <code>eu-central-1</code>.</li> <li>Grant public access to bucket and objects.  </li> <li>Create bucket.</li> </ol>"},{"location":"dynamoDB/s3/#build-and-upload-project","title":"Build and upload project","text":"<ol> <li> <p>Build application with command ng build --configuration production and the resulting built  is placed under /dist/&lt;app name&gt;.</p> </li> <li> <p>Select S3 bucket and upload those static files into the bucket.</p> </li> </ol> <p></p>"},{"location":"dynamoDB/s3/#static-website-hosting","title":"Static website hosting","text":"<ol> <li> <p>Open bucket in buckets view and go to properties.</p> </li> <li> <p>Go to section static website hosting -&gt; edit.</p> </li> <li> <p>Enable static website hosting.</p> </li> <li> <p>Specify index document: index.html </p> </li> <li>Save changes.</li> </ol> <p>Note</p> <p>When you configure a bucket as a static website, if you want your website to be public, you can grant public read access.  To make your bucket publicly readable, you must disable block public access settings for the bucket and  write a bucket policy that grants public read access.</p>"},{"location":"dynamoDB/s3/#edit-bucket-policy-optional","title":"Edit bucket policy (optional)","text":"<ol> <li>Open bucket in buckets view and go to permissions.</li> <li>Edit bucket policy.</li> <li>Copy this bucket policy (replace &lt;bucketname&gt; with real bucket name).<pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Sid\": \"PublicReadGetObject\",\n            \"Effect\": \"Allow\",\n            \"Principal\": \"*\",\n            \"Action\": \"s3:GetObject\",\n            \"Resource\": \"arn:aws:s3:::&lt;bucketname&gt;/*\"\n        }\n    ]\n}\n</code></pre> </li> </ol> <p> 4. Save Changes. 5. Go to section static website hosting and open bucket website endpoint in browser. </p>"},{"location":"environment/aws/","title":"AWS profiles setup","text":""},{"location":"environment/aws/#aws-profiles-setup","title":"AWS profiles setup","text":"<p>2 AWS profiles are about to be created locally:</p> <ul> <li>localstack one - for the local development and</li> <li>AWS cloud one - to be used for the AWS cloud deployment</li> </ul>"},{"location":"environment/aws/#profile-creation","title":"Profile creation","text":"<ol> <li> <p>Create localstack profile:</p> <pre><code>aws configure --profile localstack\nAWS Access Key ID [None]: dummy\nAWS Secret Access Key [None]: dummy\nDefault region name [None]: eu-central-1\nDefault output format [None]: json\n</code></pre> </li> <li> <p>Create jaws profile:</p> <pre><code>aws configure --profile localstack\nAWS Access Key ID [None]: &lt;actual Access Key ID of the AWS Cloud user&gt;\nAWS Secret Access Key [None]: &lt;actual Secret Access Key of the AWS Cloud user&gt;\nDefault region name [None]: eu-central-1\nDefault output format [None]: json\n</code></pre> </li> </ol>"},{"location":"environment/tools/","title":"Tools setup","text":""},{"location":"environment/tools/#tools-setup","title":"Tools setup","text":"<p>The following tooling is required to be installed for the purposes of the tutorial locally:</p> <ul> <li>git</li> <li>JDK &gt;= 11</li> <li>node</li> <li>aws cli</li> <li>docker and docker-compose, or alternatively if you're not willing to pay the license costs: podman and podman-compose</li> </ul> <p>Recommended software:</p> <ul> <li>Editor, e.g.: Visual Studio Code</li> <li>Java IDE, e.g.: Eclipse IDE or Intellij Idea Community Edition</li> <li>Postgres DB UI, e.g.: DBeaver Community Edition</li> <li>DynamoDB UI, e.g.: NoSQL Workbench for DynamoDB</li> </ul>"},{"location":"environment/tools/#windows-installation-instructions","title":"Windows installation instructions","text":"<ol> <li>Install Amazon Corretto JDK 11 Windows Installer</li> <li>Install Node.js</li> <li>Install AWS CLI</li> <li>Install Maven 3.6 (https://maven.apache.org/install.html)</li> <li>Install Linux Windows Subsystem - WSL</li> <li>Install Ubuntu distribution to WSL</li> <li>Install Podman to Ubuntu</li> <li>Install podman-compose to Ubuntu</li> </ol> <p>Note: It is possible to use Docker instead of Podman</p>"},{"location":"environment/tools/#how-to-setup-wsl-2-environment-with-ubuntu-2004","title":"How to setup WSL 2 environment with Ubuntu 20.04","text":"<p>Open windows command line and enter following commands:</p> <pre><code>     wsl --instal\n     wsl --install -d ubuntu-20.04\n     wsl --set-version Ubuntu-20.04 2\n</code></pre>"},{"location":"environment/tools/#how-to-fix-possible-issue-with-dns","title":"How to fix possible issue with DNS","text":"<p>It might happen that Ubuntu machine is unable to resolve hosts (ie. google.com) We can fix it by adding a DNS server to /etc/resolv.conf file.</p> <ol> <li> <p>Open WSL command line with Ubuntu.</p> <pre><code>wsl -d ubuntu-20.04\n</code></pre> </li> <li> <p>Edit the following file.</p> <pre><code>sudo nano /etc/resolv.conf\n</code></pre> </li> <li> <p>Add a new line with DNS server IP address.</p> <pre><code>nameserver 8.8.8.8\n</code></pre> </li> <li> <p>Press CTRL+X and Y to save the change.</p> </li> </ol>"},{"location":"environment/tools/#how-to-install-podman-and-podman-compose-in-ubuntu","title":"How to install podman and podman-compose in Ubuntu","text":"<p>Open WSL command line with Ubuntu.</p> <pre><code>    wsl -d ubuntu-20.04\n</code></pre> <ol> <li> <p>Add a repository with podman package and add PGP key:</p> <pre><code>echo \"deb https://download.opensuse.org/repositories/devel:/kubic:/libcontainers:/stable/xUbuntu_20.04/ /\" |\nsudo tee /etc/apt/sources.list.d/devel:kubic:libcontainers:stable.list\ncurl -L \"https://download.opensuse.org/repositories/devel:/kubic:\\/libcontainers:/stable/xUbuntu_20.04/Release.key\" | sudo apt-key add -\n</code></pre> </li> <li> <p>Install podman package:</p> <pre><code>sudo apt-get update\nsudo apt-get install -y podman\n</code></pre> </li> <li> <p>Install python3 and python3-pip in order to install podman-compose.</p> <pre><code>sudo apt-get install -y python3 python3-pip\n</code></pre> </li> <li> <p>Install podman-compose using python pip</p> <pre><code>sudo pip install podman-compose\n</code></pre> </li> <li> <p>Check if podman is able to run a hello-world container.:</p> <pre><code>podman run docker.io/library/hello-world\n</code></pre> </li> </ol>"},{"location":"environment/tools/#how-to-add-podman-registries-to-pull-images","title":"How to add podman registries to pull images","text":"<p>It might happen that podman is not able to pull any image because of no registry to be set. Follow these steps to add docker.io and quay.io registries.</p> <ol> <li> <p>Edit following file</p> <pre><code>sudo nano /etc/containers/registries.conf\n</code></pre> </li> <li> <p>Uncomment  (if line already exists but is commented out) or add a new line</p> <pre><code>unqualified-search-registries = [\"docker.io\", \"quay.io\"]\n</code></pre> </li> <li> <p>Press CTRL+X and Y to save the changes</p> </li> </ol>"},{"location":"environment/tools/#macos-installation-instructions","title":"MacOS installation instructions","text":"<p>Required tooling:</p> <ol> <li> <p>Homebrew is the package manager simplifying installation of the required software on MacOS. To install it (as per official instructions) run:</p> <pre><code>/bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"\n</code></pre> </li> <li> <p>Install git via Homebrew formula:</p> <pre><code>brew install git\n</code></pre> </li> <li> <p>Install Amazon Corretto JDK 11 via Homebrew:</p> <pre><code>brew tap homebrew/cask-versions\nbrew install --cask corretto11\n</code></pre> </li> <li> <p>Install node via Homebrew formula:</p> <pre><code>brew install node\n</code></pre> </li> <li> <p>Install aws cli via Homebrew formula:</p> <pre><code>brew install awscli\n</code></pre> </li> <li> <p>Install Podman via Homebrew formula:</p> <pre><code>brew install podman\n</code></pre> </li> <li> <p>Install aws cli via Homebrew formula:</p> <pre><code>brew install podman-compose\n</code></pre> </li> <li> <p>Install Maven via Homebrew formula:</p> <pre><code>brew install maven\n</code></pre> </li> </ol> <p>Recommended tooling:</p> <ol> <li> <p>Install DBeaver Community Edition via Homebrew formula:</p> <pre><code>brew install --cask visual-studio-code\n</code></pre> </li> <li> <p>Install Intellij Idea Community Edition via Homebrew formula:</p> <pre><code>brew install --cask intellij-idea-ce\n</code></pre> </li> <li> <p>Install DBeaver Community Edition via Homebrew formula:</p> <pre><code>brew install --cask dbeaver-community\n</code></pre> </li> <li> <p>Install NoSQL Workbench for DynamoDB via Homebrew formula:</p> <pre><code>brew install --cask nosql-workbench\n</code></pre> </li> </ol>"},{"location":"iam/","title":"Introduction: IAM service","text":"<p>Root account is not recommend to perform everyday tasks. </p> <p>Root user credentials are only used to perform a few account and service management tasks.</p> <p>Anyone who has root user credentials for your AWS account has unrestricted access to all the resources in your account, including billing information.</p> <p>AWS Identity and Access Management</p> <ul> <li>AWS - IAM LINK </li> </ul> <p>You can create IAM identities (users, groups, roles) and assign custom permissions sets (IAM policies) to those identities. </p> <p>This allows you to grant each user access to only the services, resources, and information that they need to perform tasks. </p> <p>Each user can also be assigned unique security credentials, access keys, and multi-factor authentication devices.</p> <p>It's a best practice to grant the least privilege for only the permissions required to perform a task.</p> <ul> <li></li> </ul>"},{"location":"iam/create/","title":"Creating user","text":"<p>Root account is not recommend to perform everyday tasks.</p> <ul> <li> <p>Search Identity and Access Management (IAM) service </p> </li> <li> <p>Click on Add User</p> </li> <li> <p>Fill properties, Credentials - pwd or access key ID</p> </li> <li> <p></p> </li> <li> <p>Click on Next, then we have to assign user to existing group (create or attach already existing in was), we will create own group</p> </li> <li> <p>Fill group name (e.g. Developers) and attach permission to group - e.g. - AdministratorAccess</p> </li> <li> <p>In next page there we can set tags to our new user. It is optional, sorting in logs can be easier via tags.</p> </li> <li> <p>Review user and create.</p> </li> <li> <p></p> </li> <li> <p>We have created user and now it's time to send credentials to person, who will be use user account</p> </li> <li> <p>Email login instructions - fill person email and person will receive email instruction how to log in</p> </li> <li> <p>Password - root user have default password for account, It is valid only once - during first login. User will change it first successful login</p> </li> </ul>"},{"location":"iam/mfa/","title":"Setup multi factor authentication","text":"<p>Having multi-factor authentication (MFA) for the  user improves security for this account.</p> <p>By default, every account is secured by username and password. In case of identity theft, attacker can easily use aws services.</p> <p>MFA in provided in aws by physical device, also called something they have (biometric devices). Login to aws console is finally done by username with password and MFA. </p> <p>Enable MFA for your account</p> <ul> <li> <p>Go to IAM dashboard</p> </li> <li> <p>Dashboard will alert you that you have not installed MFA , click on \"Add MFA\"</p> </li> <li> <p></p> </li> <li> <p>Find section - \"Multi-factor authentication (MFA) and assign MFA device</p> </li> <li> <p>Choose Virtual MFA device - app installed in your mobile device which will generate tokens.</p> </li> <li> <p>Choose one from the list of compatible applications and install it. (i personally use authy)</p> </li> <li> <p>Scan qr by mobile and insert two tokens to fields</p> </li> <li> <p></p> </li> </ul>"},{"location":"iam/recap/","title":"Recap","text":""},{"location":"iam/recap/#what-has-been-done-and-what-has-we-achieved","title":"What has been done and what has we achieved?","text":"<ol> <li>Create new account</li> <li>Create group</li> <li>Assign permission to group</li> <li>Setup MFA for account</li> </ol>"},{"location":"lambda/","title":"Introduction: Rebuild - jAWS Lambda with DynamoDB","text":"<p>All the resources will be created in Frankfurt (eu-central-1) region.</p> <p></p>"},{"location":"lambda/cleanup/","title":"Cleanup","text":"<ol> <li>First empty the S3 bucket, that contains the client source files manually (via AWS Console)</li> <li> <p>Then cleanup all the resources provisioned using SAM run:</p> <pre><code>sam delete --no-prompts --profile jaws --stack-name sam-jaws --region eu-central-1\n</code></pre> <p>Note</p> <p>The deletion of the CloudFront distribution takes longer, than sam delete command is willing to wait,          therefor execution of the delete command might fail, but resources would be dropped.         To check it, either use AWS Console -&gt; CloudFormation section, or run the command repeately.</p> </li> </ol>"},{"location":"lambda/cloud_watch/","title":"Logs analysis using CloudWatch","text":""},{"location":"lambda/cloud_watch/#display-the-application-logfiles-using-cloudwatch","title":"Display the application logfiles using CloudWatch","text":"<ul> <li>Go to Amazon CloudWatch service</li> <li>Open Logs insight and for the log groups pick: /aws/lambda/sam-jaws/... group, potentially you can expand limit to 10000 if you need to.</li> <li>Hit Run Query button afterwards (please note there is an option in the upper right corner to change the time range of the displayed logs) </li> </ul>"},{"location":"lambda/iam/","title":"IAM Access Key Setup","text":"<p>To setup the access key, 2 steps are needed:</p> <ul> <li>in the AWS Console generate the access key</li> <li>setup the access key for local usage</li> </ul>"},{"location":"lambda/iam/#generating-access-keys-for-user","title":"Generating access keys for user","text":"<p>To generate access keys for the user created in: , navigate in AWS console to:</p> <ul> <li>Identity and Access Management (IAM) service </li> <li>Click Access Management -&gt; Users   </li> <li>Pick the user of your choice</li> <li>Navigate to Security credentials tab</li> <li>Click Create Access Key button   </li> <li>Click: <code>Command Line Interface (CLI)</code> and mark checkbox: <code>I understand the above recommendation and want to proceed to create an access key.</code> </li> <li>Click Next</li> <li>Create tag of your choice, e.g.: JAWS and click Next   </li> <li>Copy/note the key and value:   </li> <li>Click done button</li> <li>Once done, it should be listed in the user's access keys section:   </li> </ul>"},{"location":"lambda/iam/#local-setup","title":"Local Setup","text":"<ul> <li>open <code>~/.aws/credentials</code> file</li> <li>add previously copied Access key and Secret access keys to new AWS profile called: <code>jaws</code> (please note region as well as output format would be set):         <pre><code>[jaws]\naws_access_key_id=&lt;Access key&gt;\naws_secret_access_key=&lt;Secret access key&gt;\nregion=eu-central-1\noutput=json\n</code></pre></li> <li>check keys are working OK, by executing sample aws command (for listing S3 buckets):         <pre><code>aws s3 ls --profile jaws\n</code></pre></li> </ul>"},{"location":"lambda/localstack/","title":"Running localy (using localstack)","text":""},{"location":"lambda/localstack/#checkout-project-from-githubibmcom-and-switch-to-refactor_lambda-branch","title":"Checkout project from github.ibm.com and switch to refactor_lambda branch","text":"<pre><code>git clone git@github.ibm.com:jAWS/ToDo-app.git\ncd ToDo-app\ngit switch refactor_lambda\ngit status\n</code></pre> <p>Result:</p> <pre><code>  On branch refactor_lambda\n  Your branch is up to date with 'origin/refactor_lambda'.\n</code></pre>"},{"location":"lambda/localstack/#starting-localstack","title":"Starting Localstack","text":"<p>To be able to run the app locally, run localstack using docker-compose (alternatively run podman-compose cmd instead of docker-compoose) first:</p> <pre><code>cd docker-compose\ndocker-compose up\n</code></pre>"},{"location":"lambda/localstack/#building-and-deploying-the-backend-lambda-using-sam-local","title":"Building and deploying the backend lambda (using SAM local)","text":"<ol> <li> <p>To build run: </p> <pre><code>cd lambda\nsam build\n</code></pre> </li> <li> <p>To deply run:</p> <pre><code>samlocal deploy --resolve-s3 --stack-name sam-app-demo --capabilities CAPABILITY_IAM --no-fail-on-empty-changeset --region eu-central-1 --parameter-overrides DynamoDBEndpointUrl=localhost.localstack.cloud:4566\n</code></pre> </li> <li> <p>To prove deployment suceeded, run:</p> <pre><code>curl http://127.0.0.1:4566/restapis/&lt;replace with random prefix from TutorialsApi output&gt;/Prod/_user_request_/api/tutorials\n</code></pre> </li> </ol>"},{"location":"lambda/localstack/#building-and-running-the-frontend-app","title":"Building and running the frontend app","text":"<p>TODO</p>"},{"location":"lambda/recap/","title":"Recap","text":""},{"location":"lambda/recap/#what-has-been-done-and-what-has-we-achieved","title":"What has been done and what has we achieved?","text":"<ol> <li>Cloned the project from repo and refactor_lambda branch</li> <li>Used AWS Serverless Application Model (SAM) to deploy ToDo application as serverless </li> <li>Copied client app to S3 bucket to host frontend</li> <li>Used CloudWatch to analyze logs</li> </ol>"},{"location":"lambda/s3/","title":"S3","text":""},{"location":"lambda/s3/#create-bucket","title":"Create bucket","text":"<p>1.) Go to Amazon S3 service</p> <p>2.) Create bucket</p> <p>3.) Choose uniqe bucket name</p> <p>4.) AWS Region = <code>eu-central-1</code></p> <p>5.) Grant public access to bucket and objects</p> <p></p> <p>6.) Other options remain default</p> <p>7.) Create bucket</p>"},{"location":"lambda/s3/#edit-bucket-policy","title":"Edit bucket policy","text":"<p>1.) Open bucket in buckets view and go to permissions</p> <p>2.) Edit bucket policy</p> <p>3.) Copy this bucket policy (replace &lt;bucketname&gt; with real bucket name)</p> <pre><code>    {\n        \"Version\": \"2012-10-17\",\n        \"Statement\": [\n            {\n                \"Sid\": \"PublicReadGetObject\",\n                \"Effect\": \"Allow\",\n                \"Principal\": \"*\",\n                \"Action\": \"s3:GetObject\",\n                \"Resource\": \"arn:aws:s3:::&lt;bucketname&gt;/*\"\n            }\n        ]\n    }\n</code></pre> <p></p>"},{"location":"lambda/s3/#static-website-hosting","title":"Static website hosting","text":"<p>1.) Open bucket in buckets view and go to properties</p> <p>2.) Go to section static website hosting -&gt; edit</p> <p>3.) Enable static website hosting</p> <p>4.) Specify index document: index.html</p> <p></p> <p>5.) Save changes</p> <p>6.) Go to section static website hosting and copy website endpoint</p> <p>7.) Paste it to client.origin in application.properties</p>"},{"location":"lambda/sam/","title":"AWS Serverless Application Model (SAM)","text":"<p>AWS SAM ( https://aws.amazon.com/serverless/sam/ ):</p> <p>The AWS Serverless Application Model (SAM) is an open-source framework for building serverless applications. It provides shorthand syntax to express functions, APIs, databases, and event source mappings. With just a few lines per resource, you can define the application you want and model it using YAML. During deployment, SAM transforms and expands the SAM syntax into AWS CloudFormation syntax, enabling you to build serverless applications faster.</p>"},{"location":"lambda/sam/#retrieve-project-sources","title":"Retrieve project sources","text":"<p>Clone project from github.ibm.com and switch to refactor_lambda branch</p> <pre><code>git clone git@github.ibm.com:jAWS/ToDo-app.git\ncd ToDo-app\ngit switch refactor_lambda\ngit status\n</code></pre> <p>Result:</p> <pre><code>  On branch refactor_lambda\n  Your branch is up to date with 'origin/refactor_lambda'.\n</code></pre>"},{"location":"lambda/sam/#build-and-deploy-using-sam","title":"Build and deploy (using SAM)","text":"<ol> <li> <p>To build run: </p> <pre><code>cd lambda\nsam build\n</code></pre> </li> <li> <p>To deply run:</p> <pre><code>sam deploy --profile jaws --resolve-s3 --stack-name sam-jaws --capabilities CAPABILITY_IAM --no-fail-on-empty-changeset --region eu-central-1\n</code></pre> </li> <li> <p>Provisioning itself is handled by the CloudFormation. To see the progress, check the Cloudformation service -&gt; stack: <code>sam-jaws</code>, the tab: Events.     </p> </li> <li> <p>To see the provisioned resources, navigate to tab resources:    </p> </li> </ol>"},{"location":"lambda/sam/#build-client","title":"Build client","text":"<ol> <li>Navigate to <code>angilar-11-client</code> folder</li> <li>Build application with command ng build --configuration production </li> <li>Build result will be in folder dist </li> </ol>"},{"location":"lambda/sam/#upload-client","title":"Upload client","text":"<ol> <li>Go to S3 bucket created by SAM (see the bucket name in the console output of the <code>sam deploy</code> command)</li> <li>Upload files </li> <li>Upload -&gt; add files </li> <li>Navigate to dist folder -&gt; Angular11Crus -&gt; select all files</li> <li>Upload files</li> </ol>"},{"location":"lambda/sam/#check-api-exposed","title":"Check API exposed","text":"<p>In the browser navigate to URL shown in the console output of the <code>sam deploy</code> command e.g.: <pre><code>CloudFormation outputs from deployed stack\n---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\nOutputs\n---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\nKey                 TutorialsApi\nDescription         API Gateway endpoint URL for Get Tutorials function\nValue               https://1i3jgli6sg.execute-api.eu-central-1.amazonaws.com/Prod/api/tutorials\n\nKey                 CloudFrontUrl\nDescription         CloudFront URL\nValue               https://d37c32w4ljwkhq.cloudfront.net\n\nKey                 DynamoDbTable\nDescription         DynamoDB Table where tasks are stored\nValue               sam-jaws-TutorialsTable-1B0RGCIQUGB38\n\nKey                 ClientS3Bucket\nDescription         S3 bucket for client site publishing\nValue               sam-jaws-s3bucket-fxrwbod4iz86\n---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n\nSuccessfully created/updated stack - sam-jaws in eu-central-1\n</code></pre></p> <p>So in my case: https://d37c32w4ljwkhq.cloudfront.net</p>"},{"location":"postgreSQL/","title":"Introduction","text":"<p>All the resources will be created in Frankfurt (eu-central-1) region.</p> <p></p>"},{"location":"postgreSQL/beanstalk/","title":"Elastic Beanstalk","text":""},{"location":"postgreSQL/beanstalk/#high-level-view","title":"High-level view","text":""},{"location":"postgreSQL/beanstalk/#create-environment","title":"Create Environment","text":"<ol> <li>Open Elastic Beanstalk console at https://console.aws.amazon.com/elasticbeanstalk/.</li> <li>On the application overview page, choose Create a new environment.</li> <li>For environment tier, choose the Web server environment.</li> <li>Enter an application name.</li> <li>Choose platform Java. </li> <li>Application code \u2013 choose \u201cUpload your code\u201d.</li> <li>Upload builded application (jar file).  </li> <li>In section Presets choose High availability and choose Next.</li> <li>The Configure service access page displays.</li> <li>Choose Create and use new service role for Service Role.</li> <li>Enter Service role name.</li> <li>If the EC2 instance profile dropdown list doesn't list any values to choose from,      Choose View permission details. This displays under the EC2 instance profile dropdown list</li> <li>A modal window titled View instance profile permissions displays. This window lists the managed profiles that you'll need to      attach to the new EC2 instance profile that you create. It also provides a link to launch the IAM console.</li> <li>Choose the IAM console link displayed at the top of the window.</li> <li>In the IAM console navigation pane, choose Roles.</li> <li>Choose Create role.</li> <li>Under Trusted entity type, choose AWS service.</li> <li>Under Use case, choose EC2.</li> <li>Choose Next.</li> <li> <p>Attach the appropriate managed policies. Scroll in the View instance profile permissions modal window to see the managed policies.      The policies are also listed here:</p> <pre><code>AWSElasticBeanstalkWebTier\n\nAmazonDynamoDBFullAccess\n\nAmazonRDSDataFullAccess\n\nSecretsManagerReadWrite\n\nCloudWatchFullAccess\n</code></pre> </li> <li> <p>Choose Next and Enter a name for the role.</p> </li> <li>Choose Create role.</li> <li>Return to the Elastic Beanstalk console window that is open.</li> <li>Close the modal window View instance profile permissions.</li> <li> <p>Choose refresh icon, next to the EC2 instance profile dropdown list.     This refreshes the dropdown list, so that the Role you just created will display in the dropdown list.</p> <p></p> </li> <li> <p>The default settings remain in  Set up networking, database, and tags step.</p> </li> <li>Step Configure instance traffic and scaling.</li> <li>In Section Capacity set Environment type to Load balanced.</li> <li> <p>Set Minimum number of instances to 2 and maximum for instances to 3.</p> <p></p> </li> <li> <p>Set Instance types: t2.micro.</p> </li> <li>Set Availability Zones: Any 1.</li> <li> <p>Set Placement: eu-central-1.</p> <p></p> </li> <li> <p>In section Load Balancer Type choose load balancer type Application Load Balancer.</p> </li> <li> <p>In part Processes change this values:</p> <ul> <li>Port      : 8080</li> <li>HTTP code : 200-299</li> <li>Health check path : /api/tutorials </li> </ul> </li> <li> <p>Choose Next.</p> </li> <li>Step Configure updates, monitoring, and logging.</li> <li> <p>In Section Health reporting set System to Basic.</p> <p> </p> </li> <li> <p>In Section Rolling updates and deployments set Deployment policy to All at once.</p> </li> <li>Set Ignore health check to False</li> <li> <p>Enable streaming of the logs to Cloudwatch, by setting the checkbox      (We will need this for later analysis in in part Logs analysis using CloudWatch):</p> <p></p> </li> <li> <p>In section Environment properties introduce new environment variable: SPRING_PROFILES_ACTIVE with value: prod.          </p> </li> <li> <p>Check Review page and submit.</p> </li> </ol> <p>Note</p> <p>This will take a few minutes. Beanstalk will create EC2 instance, Elastic load balancer,  Auto Scaling Group, Target group, Security Group, CloudWatch Alarm. </p>"},{"location":"postgreSQL/beanstalk/#security-group-for-ec2-rds-connectivity","title":"Security group for EC2 -&gt; RDS connectivity","text":"<p>Security groups serve the purpose of firewalls in the AWS. To enable connectivity between EC2 instances and RDS database, following steps are to be performed:</p> <ol> <li> <p>Go to RDS Service.</p> </li> <li> <p>Select Postgres DB created in the previous part: PostgreSQL .</p> </li> <li> <p>In the \"Connected compute resources\" section click \"Setup EC2 connection\" button. </p> </li> <li>Choose the respective EC2 instance in the dropdown and click \"Continue\" button. </li> <li>Click \"Confirm and set up\" button afterwards.</li> </ol>"},{"location":"postgreSQL/cleanup/","title":"Cleanup","text":"<ol> <li>Remove PostgreSQL database</li> <li>Remove all deployed objects in S3 and delete bucket</li> <li>Remove Beanstalk Environment</li> <li>Remove all secrets from Secret Manager</li> <li>Disable and delete CloudFront distribution</li> </ol>"},{"location":"postgreSQL/cloud_watch/","title":"Logs analysis using CloudWatch","text":""},{"location":"postgreSQL/cloud_watch/#high-level-view","title":"High-level view","text":""},{"location":"postgreSQL/cloud_watch/#display-the-application-logfiles-using-cloudwatch","title":"Display the application logfiles using CloudWatch","text":"<ul> <li>Go to Amazon CloudWatch service</li> <li>Open Logs insight and for the log groups pick: /aws/beanstalk/.../var/log/web.stdout.log and in the query change limit to 2000.</li> <li>Hit Run Query button afterwards (please note there is an option in the upper right corner to change the time range of the displayed logs) </li> </ul>"},{"location":"postgreSQL/cloudfront/","title":"CloudFront","text":""},{"location":"postgreSQL/cloudfront/#high-level-view","title":"High-level view","text":""},{"location":"postgreSQL/cloudfront/#create-cloudfront","title":"Create CloudFront","text":"<ol> <li>Open the Amazon S3 console at https://console.aws.amazon.com/cloudfront/.</li> <li>Create distribution.</li> <li>Set origin domain for client - find S3 Bucket.</li> <li>Origin access set to Origin access control settings.</li> <li>For Origin access control Create new control setting.    CloudFront will provide us policy statement for bucket policy after creating the distribution.</li> <li>For Allowed HTTP methods choose GET, HEAD, OPTIONS, PUT, POST, PATCH, DELETE.    </li> <li>In section Cache key and origin requests use recommended cache policy and disable caching and set     Origin request policy to CORS-S3Origin.    </li> <li>Turn off Web Application Firewall (WAF).  </li> <li>Default root object set to index.html.</li> <li>Create distribution.</li> <li>After distribution is created pop up window offer us new bucket policy. Copy it and replace existing policy in our S3 bucket.  </li> <li>In Origins section create origin.</li> <li>Set origin domain for application load balancer - find ELB.  </li> <li>Create origin.</li> <li>In Behaviours section create behaviour.</li> <li>Set Path pattern to /api/*</li> <li>Set Origin and origin groups to ELB.</li> <li>For Allowed HTTP methods choose GET, HEAD, OPTIONS, PUT, POST, PATCH, DELETE.</li> <li>In section Cache key and origin requests use recommended cache policy and disable caching and set      Origin request policy to AllViewer. </li> <li>After behaviour is created distribution is redeployed. After succesfull deployment open website with distribution domain name.  (General section)</li> </ol>"},{"location":"postgreSQL/localstack/","title":"Building the projects","text":""},{"location":"postgreSQL/localstack/#retrieve-project-sources","title":"Retrieve project sources","text":"<p>Clone project from github.ibm.com and switch to lift_and_switch branch</p> <pre><code>git clone git@github.ibm.com:jAWS/ToDo-app.git\ncd ToDo-app\ngit switch lift_and_switch\ngit status\n</code></pre> <p>Result:</p> <pre><code>  On branch lift_and_shift\n  Your branch is up to date with 'origin/lift_and_shift'.\n</code></pre>"},{"location":"postgreSQL/localstack/#building-the-backend-app","title":"Building the backend app","text":"<ol> <li> <p>To build run: </p> <pre><code>cd spring-boot-server\nmvn package\n</code></pre> </li> <li> <p>Built artifact is present in: <code>target/*.jar</code> file</p> </li> </ol>"},{"location":"postgreSQL/localstack/#building-the-frontend-app","title":"Building the frontend app","text":"<ol> <li> <p>Have NodeJS installed (LTS version is preffered, 18.12.1 is latest LTS). I have 16.15.0, this is version about 6 months old, should work just fine.</p> </li> <li> <p>Install angular CLI globaly with npm (node package manager) -&gt; if you have yarn, you can use that.</p> <pre><code>npm install -g @angular/cli\n</code></pre> </li> <li> <p>Clone GitHub repository and navigate to angular app application is located in angular-11-client/ folder.</p> </li> <li> <p>Install application\u2019s dependencies</p> <pre><code>npm install\n</code></pre> </li> <li> <p>To build application, simply run:</p> <pre><code>ng build\n</code></pre> <p>You can also run ng build --prod, which will produce optimized production bundle.</p> <p>Difference between them is first one also has map files to allow tracing errors and issues to specific code.</p> <p>Production bundle will just produce mostly useless stacktrace, as compiled JS code is optimized for performance.</p> </li> </ol> <p>Produced bundle can be found in dist/Angular11Crud/ folder, you can just copy-paste this bundle into AWS S3 bucket more info can be found here https://levelup.gitconnected.com/learn-how-to-create-and-deploy-the-angular-application-to-aws-serverless-s3-81f8a838b563</p>"},{"location":"postgreSQL/localstack/#running-localy-optional-for-next-steps","title":"Running localy (optional for next steps)","text":""},{"location":"postgreSQL/localstack/#starting-localstack-postgresql","title":"Starting Localstack + PostgreSQL","text":"<p>To be able to run the app locally, run Localstack + PostgreSQL using docker-compose (alternatively run podman-compose cmd instead of docker-compoose) first:</p> <pre><code>cd docker-compose\ndocker-compose up\n</code></pre>"},{"location":"postgreSQL/localstack/#running-the-backend-app-locally","title":"Running the backend app locally","text":"<ol> <li> <p>Run (in project root folder): </p> <pre><code>cd spring-boot-server\nmvn spring-boot:run\n</code></pre> </li> <li> <p>To prove deployment suceeded, run:</p> <pre><code>curl http://localhost:8080/api/tutorials\n</code></pre> </li> </ol>"},{"location":"postgreSQL/localstack/#running-the-frontend-app","title":"Running the frontend app","text":"<ol> <li> <p>Run:</p> <pre><code>npm run start\n</code></pre> <p>(which runs ng serve internally)</p> <p>OR </p> <pre><code>ng serve\n</code></pre> </li> </ol> <p>Application is then started on port defined in angular.json (by default 4200).</p>"},{"location":"postgreSQL/part_secrets_manager/","title":"Secrets Manager","text":""},{"location":"postgreSQL/part_secrets_manager/#high-level-view","title":"High-level view","text":""},{"location":"postgreSQL/part_secrets_manager/#storage-of-the-postgres-password-in-aws-secrets-manager","title":"Storage of the Postgres password in AWS Secrets Manager","text":"<ol> <li>Open the Amazon Secrets Manager service console at https://console.aws.amazon.com/secretsmanager/.</li> <li>Click \"Store a new secret\" button.</li> <li>Choose Secret type of: \"Credentials for Amazon RDS database\".</li> <li>Provide previously created DB Username and Password.</li> <li>Select the Database, that was created previously from the list.</li> <li>Click Next. </li> <li>Enter Secret name: rds-db-credentials/postgres-jaws-tutorial (please note to keep the prefix: rds-db-credentials/ , as the Policy we'll attach to EC2 instance in the later step: Elastic Beanstalk called: AmazonRDSDataFullAccess* expects the naming convention) </li> <li>Click Next.</li> <li>Disable Automatic rotation. </li> <li>Click Next and Store secret.</li> </ol>"},{"location":"postgreSQL/postgresql/","title":"RDS PostgreSQL","text":""},{"location":"postgreSQL/postgresql/#high-level-view","title":"High-level view","text":""},{"location":"postgreSQL/postgresql/#creating-a-postgresql-db-instance","title":"Creating a PostgreSQL DB instance","text":"<ol> <li>Sign in to the AWS Management Console and open the Amazon RDS console at https://console.aws.amazon.com/rds/.</li> <li>In the upper-right corner of the Amazon RDS console, choose the AWS Region in which you want to create the DB instance.</li> <li>In the navigation pane, choose Databases.</li> <li>Choose Create database and make sure that Standard create is chosen.</li> <li>In Engine options, choose PostgreSQL.</li> <li>For Templates, choose Free tier. </li> <li>For DB instance identifier, enter a name for the DB instance, or leave the default name.</li> <li>For Master username, enter a name for the master user, or leave the default name (postgres).</li> <li>Enter the same password in Master password and Confirm master password.</li> <li>In part Connectivity, set public access to Yes. </li> <li>Create database.</li> </ol> <p>Note</p> <p>On the RDS console, the details for new DB instance appear.  The DB instance has a status of Creating until the DB instance is ready to use.  When the state changes to Available, you can connect to the DB instance.  Depending on the DB instance class and the amount of storage, it can take up to 20 minutes before the new instance is available.</p> <p></p>"},{"location":"postgreSQL/postgresql/#provide-access-to-your-db-instance-in-your-vpc-optional","title":"Provide access to your DB instance in your VPC (optional)","text":"<p>Note</p> <p>VPC security groups provide access to DB instances in a VPC. They act as a firewall for the associated DB instance,  controlling both inbound and outbound traffic at the DB instance level.  DB instances are created by default with a firewall and a default security group that protect the DB instance.</p> <p>Note</p> <p>Before you can connect to your DB instance, you must add rules to a security group that enable you to connect.  Use your network and configuration information to create rules to allow access to your DB instance.</p> <ol> <li>In the navigation pane of your database, choose Connectivity &amp; security.</li> <li>Open security group which is associated with database instance. </li> <li>In Inbound rules, choose Edit inbound rules.</li> <li>Add rule:<pre><code>Type: All traffic\n\nSource: My IP\n</code></pre> </li> </ol> <p></p>"},{"location":"postgreSQL/postgresql/#connecting-to-a-postgresql-db-instance-optional","title":"Connecting to a PostgreSQL DB instance (optional)","text":"<ol> <li>Launch the DBeaver application on your client computer.</li> <li>Choose Database -&gt; New Database Conection from the menu.</li> <li>Choose PostgreSQL database driver.</li> <li>Enter the DB instance endpoint (for example, database-1.123456789012.us-west-1.rds.amazonaws.com) in the Host box.</li> <li>Enter the port you assigned to the DB instance for Port. </li> <li>Enter the user name and user password that you entered when you created the DB instance for Username and Password.</li> <li>Choose OK.</li> </ol> <p>Note</p> <p>In some cases, you might have difficulty connecting to the DB instance. If so, the problem is most often with the access rules that you set up.  These reside in the security group that you assigned to the DB instance.</p> <p>Note</p> <p>If your DB instance is publicly accessible, make sure its associated security group has inbound rules for the IP addresses  that you want to access it. If your DB instance is private,  make sure its associated security group has inbound rules for the security group of each resource to access it.</p>"},{"location":"postgreSQL/recap/","title":"Recap","text":""},{"location":"postgreSQL/recap/#what-has-been-done-and-what-has-we-achieved","title":"What has been done and what has we achieved?","text":"<ol> <li>Cloned the project from repo and lift_and_shift branch.</li> <li>Created a PostgreSQL database in AWS.</li> <li>Created S3 bucket to host frontend.</li> <li>Configured Secret manager to store credentials for a database.</li> <li>Used Beanstalk to for easy application deployment.</li> <li>Used CloudFront for distribution static and dynamic web content.</li> <li>Used CloudWatch to analyze logs.</li> </ol>"},{"location":"postgreSQL/s3/","title":"S3","text":""},{"location":"postgreSQL/s3/#high-level-view","title":"High-level view","text":""},{"location":"postgreSQL/s3/#create-bucket","title":"Create bucket","text":"<ol> <li>Open the Amazon S3 console at https://console.aws.amazon.com/s3/.</li> <li>Create bucket.</li> <li>Choose unique bucket name.</li> <li>AWS Region = <code>eu-central-1</code>.</li> <li>Grant public access to bucket and objects.  </li> <li>Create bucket.</li> </ol>"},{"location":"postgreSQL/s3/#build-and-upload-project","title":"Build and upload project","text":"<ol> <li> <p>Build application with command ng build --configuration production and the resulting built  is placed under /dist/&lt;app name&gt;.</p> </li> <li> <p>Select S3 bucket and upload those static files into the bucket.</p> </li> </ol> <p></p>"},{"location":"postgreSQL/s3/#static-website-hosting","title":"Static website hosting","text":"<ol> <li> <p>Open bucket in buckets view and go to properties.</p> </li> <li> <p>Go to section static website hosting -&gt; edit.</p> </li> <li> <p>Enable static website hosting.</p> </li> <li> <p>Specify index document: index.html </p> </li> <li>Save changes.</li> </ol> <p>Note</p> <p>When you configure a bucket as a static website, if you want your website to be public, you can grant public read access.  To make your bucket publicly readable, you must disable block public access settings for the bucket and  write a bucket policy that grants public read access.</p>"},{"location":"postgreSQL/s3/#edit-bucket-policy-optional","title":"Edit bucket policy (optional)","text":"<ol> <li>Open bucket in buckets view and go to permissions.</li> <li>Edit bucket policy.</li> <li>Copy this bucket policy (replace &lt;bucketname&gt; with real bucket name).<pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Sid\": \"PublicReadGetObject\",\n            \"Effect\": \"Allow\",\n            \"Principal\": \"*\",\n            \"Action\": \"s3:GetObject\",\n            \"Resource\": \"arn:aws:s3:::&lt;bucketname&gt;/*\"\n        }\n    ]\n}\n</code></pre> </li> </ol> <p> 4. Save Changes. 5. Go to section static website hosting and open bucket website endpoint in browser. </p>"},{"location":"support/","title":"Local Environment support","text":"<p>Helpful page for fixing issues during set up local environment.</p>"},{"location":"support/#checking-ip-route-for-wsl-setting-db-address-for-backend","title":"Checking ip route for WSL ( setting DB address for backend  )","text":"<p>In WSL run command  <pre><code> ip addr show eth0 | grep -oP '(?&lt;=inet\\s)\\d+(\\.\\d+){3}'\n</code></pre></p> <p>or  <pre><code> ip route \n</code></pre></p> <p> and then set address to application properties of your backend (spring-boot-server)</p> <pre><code>spring.datasource.url= jdbc:postgresql://172.19.150.141:5432/jawstutorial\n</code></pre>"},{"location":"support/#missing-values-in-registries","title":"Missing values in registries","text":"<p>Error message: <pre><code>Error: short-name \"postgres:14-alpine\" did not resolve to an alias and no unqualified-search registries are defined in \"/etc/containers/registries.conf\"\n</code></pre> Fix:</p> <ul> <li>In WSL run: <code>sudo vi /etc/containers/registries.conf</code></li> <li>Add to registries.conf this line :  <code>registries = ['docker.io', 'quay.io']</code></li> </ul>"},{"location":"support/#erro0000-error-joining-network-namespace-for-container","title":"ERRO[0000] error joining network namespace for container","text":"<p>Error message: <pre><code>podman ps\nERRO[0000] error joining network namespace for container 60cb48303fd3dfd191b6924aae4b2c7865d5bb4ef39f573fbe0086b9c78f5ad8: error retrieving network namespace at /tmp/podman-run-1000/netns/cni-fcd7c7fb-b057-8bb4-59a1-d492664aeda4\n</code></pre> Fix: <pre><code>podman rm 60cb48303fd3dfd191b6924aae4b2c7865d5bb4ef39f573fbe0086b9c78f5ad8  -f </code></pre></p>"},{"location":"support/#issue-while-starting-podman-compose-on-macos","title":"Issue while starting podman-compose on MacOS","text":"<p>You need to init+start the podman machine first, via: <pre><code>podman machine init --cpus=2 --disk-size=60 --memory=8096 -v $HOME:$HOME\n</code></pre> and then <pre><code>podman machine start\n</code></pre></p>"},{"location":"support/#permission-denied-publickey","title":"Permission denied (publickey).","text":"<p>Your ssh public key is not in your Github account https://github.ibm.com/settings/keys</p> <p>Follow official documentation: https://docs.github.com/enterprise-server@3.3/articles/generating-an-ssh-key/</p>"},{"location":"support/#docker-compose-in-ubuntu-for-windows","title":"Docker-compose in Ubuntu for windows","text":"<p>In docker App you need set properties like that</p> <p></p>"},{"location":"support/#skip-podmandocker-compose-and-install-postgresql-directly-to-wsl","title":"Skip podman/docker compose and install postgreSQL directly to WSL","text":"<pre><code>sudo apt install postgresql\nsudo service postgresql start\nsudo -u postgres psql\n\\password postgres\nCREATE DATABASE jawstutorial;\nexit\n</code></pre>"},{"location":"support/#installing-wsl2-ubuntu-to-eliminate-cannot-re-exec-process-error","title":"Installing WSL2 + Ubuntu to eliminate \"cannot re-exec process\" error :","text":"<pre><code>1.  Turn windows features on and off \u2013 select Virtual Machine Platform\n2.  Install wsl -       PowerShell:     wsl --install\n3.  Set default wsl2 -  PowerShell:     wsl --set-default-version 2\n4.  Install ubuntu -    PowerShell:     wsl --install -d ubuntu\n5.  Upgrade ubuntu -    Ubuntu:     sudo apt update; sudo apt upgrade\n6.  Install podman -    Ubuntu:     sudo apt install podman\n7.  Create postgres pod -   Ubuntu: cd ToDo-app/docker-compose; podman play kube postgres.yaml\n</code></pre>"},{"location":"vanilla/localstack/","title":"Running localy","text":""},{"location":"vanilla/localstack/#getting-project-from-githubibmcom","title":"Getting project from github.ibm.com","text":"<pre><code>git clone git@github.ibm.com:jAWS/ToDo-app.git\ncd ToDo-app\ngit status\n</code></pre> <p>Result:</p> <pre><code>  On branch vanilla\n  Your branch is up to date with 'origin/vanilla'.\n</code></pre>"},{"location":"vanilla/localstack/#starting-postgresql","title":"Starting PostgreSQL","text":"<p>To be able to run the app locally, run PostgreSQL using docker-compose (alternatively run podman-compose cmd instead of docker-compose) first:</p> <pre><code>cd docker-compose\ndocker-compose up\n</code></pre>"},{"location":"vanilla/localstack/#building-and-deploying-the-backend-app","title":"Building and deploying the backend app","text":"<ol> <li> <p>To build run: </p> <pre><code>cd spring-boot-server\nmvn spring-boot:run\n</code></pre> </li> <li> <p>To prove deployment suceeded, run:</p> <pre><code>curl http://localhost:8080/api/tutorials\n</code></pre> </li> </ol>"},{"location":"vanilla/localstack/#building-and-running-the-frontend-app","title":"Building and running the frontend app","text":"<ol> <li> <p>Have NodeJS installed (LTS version is preffered, 18.12.1 is latest LTS). I have 16.15.0, this is version about 6 months old, should work just fine.</p> </li> <li> <p>Install angular CLI globaly with npm (node package manager) -&gt; if you have yarn, you can use that.</p> <pre><code>npm install -g @angular/cli\n</code></pre> </li> <li> <p>Clone GitHub repository and navigate to angular app application is located in angular-11-client/ folder.</p> </li> <li> <p>Install application\u2019s dependencies</p> <pre><code>npm install\n</code></pre> </li> <li> <p>If you want to run app locally, you can start application by running:</p> <pre><code>npm run start\n</code></pre> <p>(which runs ng serve internally)</p> <p>OR </p> <pre><code>ng serve\n</code></pre> <p>Application is then started on port defined in angular.json (by default 4200).</p> </li> <li> <p>To build application, simply run:</p> <pre><code>ng build\n</code></pre> <p>You can also run ng build --prod, which will produce optimized production bundle.</p> <p>Difference between them is first one also has map files to allow tracing errors and issues to specific code.</p> <p>Production bundle will just produce mostly useless stacktrace, as compiled JS code is optimized for performance.</p> </li> </ol> <p>Produced bundle can be found in dist/Angular11Crud/ folder, you can just copy-paste this bundle into AWS S3 bucket more info can be found here https://levelup.gitconnected.com/learn-how-to-create-and-deploy-the-angular-application-to-aws-serverless-s3-81f8a838b563</p>"},{"location":"vanilla/recap/","title":"Recap","text":""},{"location":"vanilla/recap/#what-has-been-done-and-what-has-we-achieved","title":"What has been done and what has we achieved?","text":"<ol> <li>Cloned the project from repo and vanilla branch</li> <li>Built and deployed backend of ToDo application</li> <li>Built and deployed frontend of ToDo application</li> </ol>"}]}